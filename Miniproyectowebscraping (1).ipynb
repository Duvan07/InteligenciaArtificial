{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "\n",
    "import os.path as path\n",
    "\n",
    "import re\n",
    "\n",
    "def traerLinks(url):\n",
    "    html_page = urlopen(\"https://\"+url)\n",
    "    soup = BeautifulSoup(html_page)\n",
    "    links = []\n",
    "\n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "        links.append(link.get('href'))\n",
    "\n",
    "    return links\n",
    "\n",
    "def Esnuevalaurl(url):\n",
    "    #print(url + \" entro bien\")\n",
    "    if (path.exists(\"listado.txt\")):\n",
    "       # print(\"entro si existe el archivo\")\n",
    "        listapaginas = []\n",
    "        archivo = open(\"listado.txt\", \"r\")\n",
    "        for linea in archivo.readlines():\n",
    "            listapaginas.append(linea)\n",
    "        \n",
    "        resultado=1\n",
    "        \n",
    "        for i in listapaginas:\n",
    "            if i == (url+\"\\n\"):\n",
    "                resultado=0\n",
    "                break\n",
    "        \n",
    "        archivo.close()\n",
    "        return resultado\n",
    "    else:\n",
    "        #crea un nuevo archivo ya que no existe\n",
    "        archivo = open(\"listado.txt\", \"w\")\n",
    "        archivo.close()\n",
    "        return 1\n",
    "\n",
    "def Agregarurlalista(url):\n",
    "    archivo = open(\"listado.txt\", \"a\")\n",
    "    archivo.write(url+\"\\n\")\n",
    "    archivo.close()\n",
    "\n",
    "def scraping(url):\n",
    "    print(\"Buscando referencias de la pagina ...\")\n",
    "    \n",
    "    nueva=Esnuevalaurl(url)\n",
    "    if (nueva==1):\n",
    "        \n",
    "        try:\n",
    "            html = urlopen(\"https://\"+url)\n",
    "        except HTTPError:\n",
    "            print(\"El servidor no responde\") \n",
    "        except URLError:  \n",
    "            print(\"La pagina no existe o esta mal escrita\")\n",
    "    \n",
    "        res = BeautifulSoup(html.read(),\"html5lib\")\n",
    "    \n",
    "        if res.title is None:\n",
    "            print(\"pagina no disponible\")\n",
    "        else:\n",
    "            print(\"creando archivo local ...\")\n",
    "            Agregarurlalista(url)\n",
    "            #CREA ARCHIVO DE TEXTO CON EL CONTENIDO DE LA PAGINA\n",
    "            textopagina = res.get_text()\n",
    "            archivo = open(url+\".txt\", \"a\", encoding=\"utf-8\")\n",
    "            archivo.write(textopagina+\"\\n\")\n",
    "            archivo.close()\n",
    "            print(\"archivo creado, mostrando pagina ...\")\n",
    "            print(textopagina)\n",
    "    else:\n",
    "        print(\"***pagina ya en el listado***\")\n",
    "        print(\"cargando pagina de archivo\")\n",
    "        archivo = open(url+\".txt\", \"r\", encoding=\"utf-8\")\n",
    "        for linea in archivo.readlines():\n",
    "            print(linea)\n",
    "        archivo.close()\n",
    "\n",
    "print(\"+++ WEB SCRAPING +++\")\n",
    "while(1):\n",
    "    print(\"Por favor ingrese la pagina web que desea visitar (ej: www.google.com)\")\n",
    "    pagina = input()\n",
    "    if pagina==\"limpiarlista\":\n",
    "        archivo = open(\"listado.txt\", \"w\")\n",
    "        archivo.close()\n",
    "        print(\"*listado de paginas vacio*\")\n",
    "    else:\n",
    "        scraping(pagina)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
